{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 09\n","## Natural Language Proessing\n","### Tokenization\n","This code snippet is tokenizing the given text using the Natural Language Toolkit (nltk) library in Python.  The Natural Language Toolkit (nltk) is a widely-used library in Python, specifically designed for working with human language data.\n","- Let us start by importing relevant functions and using it."],"metadata":{"id":"y4S9fedO9RxZ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vy6avzcoBQxG","outputId":"730625bb-435e-4ce3-afc8-682d0a351a43","executionInfo":{"status":"ok","timestamp":1695187104597,"user_tz":240,"elapsed":1736,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNmtO3Di70TV","outputId":"74d1f540-759e-48c7-cf56-c9bc39d3b764","executionInfo":{"status":"ok","timestamp":1695187104597,"user_tz":240,"elapsed":5,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'book', 'about', 'algorithms', '.']\n"]}],"source":["from nltk.tokenize import word_tokenize\n","corpus = 'This is a book about algorithms.'\n","\n","tokens = word_tokenize(corpus)\n","print(tokens)"]},{"cell_type":"markdown","source":["To tokenize text based on sentences, you can use the sent_tokenize function from the nltk.tokenize module."],"metadata":{"id":"6QkSCiSu8QoT"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","corpus = 'This is a book about algorithms. It covers various topics in depth.'"],"metadata":{"id":"lfgB79N09WV5","executionInfo":{"status":"ok","timestamp":1695187104597,"user_tz":240,"elapsed":3,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\n","In this example, the corpus variable contains two sentences. The sent_tokenize function takes the corpus as input and returns a list of sentences. When you run the modified code, you will get the following output:"],"metadata":{"id":"ChBYmVoG8ZX3"}},{"cell_type":"code","source":["sentences = sent_tokenize(corpus)\n","print(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-S9tCXfM9XIP","outputId":"640b51d8-0688-4d8f-dbff-f337093ab1df","executionInfo":{"status":"ok","timestamp":1695187104895,"user_tz":240,"elapsed":301,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['This is a book about algorithms.', 'It covers various topics in depth.']\n"]}]},{"cell_type":"markdown","source":["Sometimes we may need to break down large texts into paragraph-level chunks, NLTK can help with that task. It's a feature that could be particularly useful in applications such as document summarization, where understanding the structure at the paragraph level may be crucial. Tokenizing text into paragraphs might seem straightforward, but it can be complex depending on the structure and format of the text. A simple approach is to split the text by two newline characters, which often separate paragraphs in plain text documents."],"metadata":{"id":"GIiv2iLt8oVh"}},{"cell_type":"code","source":["def tokenize_paragraphs(text):\n","    # Split by two newline characters\n","    paragraphs = text.split('\\n\\n')\n","    return [p.strip() for p in paragraphs if p]\n"],"metadata":{"id":"DlSPxNCVADW3","executionInfo":{"status":"ok","timestamp":1695187104895,"user_tz":240,"elapsed":7,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Cleaning data using Python\n","Let us study some techniques used to clean data and prepare it for machine learning tasks:"],"metadata":{"id":"NMj8JHokAH06"}},{"cell_type":"code","source":["import string\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","# Make sure to download the NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b6Ovj1SAIfy","outputId":"b953a167-476d-47e2-983d-f9d27efffb20","executionInfo":{"status":"ok","timestamp":1695187104895,"user_tz":240,"elapsed":7,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Let us look into how we can clean text using Python."],"metadata":{"id":"Vi4c-jSh9Kgr"}},{"cell_type":"code","source":["def clean_text(text):\n","    \"\"\"\n","    Cleans input text by converting case, removing punctuation, numbers, white spaces, stop words and stemming\n","    \"\"\"\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Remove white spaces\n","    text = text.strip()\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = nltk.word_tokenize(text)\n","    filtered_text = [word for word in tokens if word not in stop_words]\n","    text = ' '.join(filtered_text)\n","\n","    # Stemming\n","    ps = PorterStemmer()\n","    tokens = nltk.word_tokenize(text)\n","    stemmed_text = [ps.stem(word) for word in tokens]\n","    text = ' '.join(stemmed_text)\n","\n","    return text\n"],"metadata":{"id":"a0IMfUMfAKe6","executionInfo":{"status":"ok","timestamp":1695187104896,"user_tz":240,"elapsed":7,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Let us test this function clean_text()"],"metadata":{"id":"OUrtAexf9Vw7"}},{"cell_type":"code","source":["corpus=\"7- Today, Ottawa is becoming cold again \"\n","clean_text(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YIxvQZ12ANb5","outputId":"1dd72477-2519-4257-b8b0-a71f43ab9852","executionInfo":{"status":"ok","timestamp":1695187104896,"user_tz":240,"elapsed":7,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'today ottawa becom cold'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### Understanding the term \"Document Matrix\"\n","This matrix structure allows efficient storage, organization, and analysis of large text datasets In Python, the CountVectorizer module from the sklearn library can be used to create TDM as follows:"],"metadata":{"id":"_v4pQjjNAR7e"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Define a list of documents\n","documents = [\"Machine Learning is useful\", \"Machine Learning is fun\", \"Machine Learning is AI\"]\n","\n","# Create an instance of CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the documents into a TDM\n","tdm = vectorizer.fit_transform(documents)\n","\n","# Print the TDM\n","print(tdm.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-_zkqHzATOr","outputId":"31726e46-dd1e-424b-a1e4-e58d9df71c74","executionInfo":{"status":"ok","timestamp":1695187104896,"user_tz":240,"elapsed":6,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 1 1 1]\n"," [0 1 1 1 1 0]\n"," [1 0 1 1 1 0]]\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Define a list of documents\n","documents = [\"Machine Learning enables learning\", \"Machine Learning is fun\", \"Machine Learning is useful\"]\n","\n","# Create an instance of TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the documents into a TF-IDF matrix\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","\n","# Get the feature names\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Loop over the feature names and print the TF-IDF score for each term\n","for i, term in enumerate(feature_names):\n","    tfidf = tfidf_matrix[:, i].toarray().flatten()\n","    print(f\"{term}: {tfidf}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suAKN24RAYIP","outputId":"22bb6bcc-f8c1-4e91-c5fd-80f0798c0165","executionInfo":{"status":"ok","timestamp":1695187105147,"user_tz":240,"elapsed":2,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["enables: [0.60366655 0.         0.        ]\n","fun: [0.         0.66283998 0.        ]\n","is: [0.         0.50410689 0.50410689]\n","learning: [0.71307037 0.39148397 0.39148397]\n","machine: [0.35653519 0.39148397 0.39148397]\n","useful: [0.         0.         0.66283998]\n"]}]},{"cell_type":"markdown","source":["### Implementing word embedding with Word2Vec\n","Word2Vec is a prominent method used for obtaining vector representations of words, commonly referred to as word embeddings. Rather than \"generating words,\" this algorithm creates numerical vectors that represent the semantic meaning of each word in the language."],"metadata":{"id":"QVKp4m-PAc79"}},{"cell_type":"code","source":["import gensim\n","\n","# Define a text corpus\n","corpus = [['apple', 'banana', 'orange', 'pear'],\n","          ['car', 'bus', 'train', 'plane'],\n","          ['dog', 'cat', 'fox', 'fish']]\n","\n","# Train a word2vec model on the corpus\n","model = gensim.models.Word2Vec(corpus, window=5, min_count=1, workers=4)"],"metadata":{"id":"GUwtJbxuAda-","executionInfo":{"status":"ok","timestamp":1695187105652,"user_tz":240,"elapsed":506,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(model.wv.similarity('car', 'train'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR6j8XW_AhRW","outputId":"0d24e947-84c2-49fa-82bb-2e364a9bfe99","executionInfo":{"status":"ok","timestamp":1695187105653,"user_tz":240,"elapsed":4,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.057745814\n"]}]},{"cell_type":"code","source":["print(model.wv.similarity('car', 'apple'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ig_Ys0UAi91","outputId":"f3d36147-8c72-4ea1-cbfd-99eb26d05126","executionInfo":{"status":"ok","timestamp":1695187105876,"user_tz":240,"elapsed":225,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["0.11117952\n"]}]},{"cell_type":"markdown","source":["## Case study: Restaurant review sentiment analysis\n","We will use the Yelp Reviews dataset which contains labelled reviews as positive(5 stars) or negative(1start).  We will train a model that can classify the reviews of a restaurant as negative or positive"],"metadata":{"id":"RVF8vOrQAmYV"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fW_jI1crBIyG","executionInfo":{"status":"ok","timestamp":1695187105876,"user_tz":240,"elapsed":3,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}},"outputId":"91ee6b6a-9dc4-4baf-f848-1206e017eba3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import re\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords"],"metadata":{"id":"UQXrVibsAnLl","executionInfo":{"status":"ok","timestamp":1695187107092,"user_tz":240,"elapsed":1218,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["url = 'https://storage.googleapis.com/neurals/data/2023/Restaurant_Reviews.tsv'\n","dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n","dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"d6j_jh-nAwJO","outputId":"aa8cfb4f-5397-424a-fd22-7bcad574673e","executionInfo":{"status":"ok","timestamp":1695187107332,"user_tz":240,"elapsed":244,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              Review  Liked\n","0                           Wow... Loved this place.      1\n","1                                 Crust is not good.      0\n","2          Not tasty and the texture was just nasty.      0\n","3  Stopped by during the late May bank holiday of...      1\n","4  The selection on the menu was great and so wer...      1"],"text/html":["\n","  <div id=\"df-483fd19a-728b-4323-8fb9-ab59333e7b72\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Liked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Wow... Loved this place.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Crust is not good.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Not tasty and the texture was just nasty.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Stopped by during the late May bank holiday of...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The selection on the menu was great and so wer...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-483fd19a-728b-4323-8fb9-ab59333e7b72')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-483fd19a-728b-4323-8fb9-ab59333e7b72 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-483fd19a-728b-4323-8fb9-ab59333e7b72');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b4b37e21-5b42-420b-9dbb-2f52db55282e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4b37e21-5b42-420b-9dbb-2f52db55282e')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b4b37e21-5b42-420b-9dbb-2f52db55282e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["def clean_text(text):\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    ps = PorterStemmer()\n","    text = [\n","        ps.stem(word) for word in text\n","        if not word in set(stopwords.words('english'))]\n","    text = ' '.join(text)\n","    return text\n","\n","corpus = [clean_text(review) for review in dataset['Review']]"],"metadata":{"id":"lNsNj5SpAyY8","executionInfo":{"status":"ok","timestamp":1695187112577,"user_tz":240,"elapsed":5247,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix\n","\n","# Initialize the CountVectorizer and transform the corpus\n","vectorizer = CountVectorizer(max_features=1500)\n","X = vectorizer.fit_transform(corpus).toarray()\n","\n","# Get the target labels\n","y = dataset.iloc[:, 1].values\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n","\n","# Initialize and train the Gaussian Naive Bayes classifier\n","classifier = GaussianNB()\n","classifier.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yo-qXa5D_cLU","outputId":"5a155bfa-5214-4c01-ecb8-9d2e409faac0","executionInfo":{"status":"ok","timestamp":1695187112577,"user_tz":240,"elapsed":3,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[[55 42]\n"," [12 91]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xG8OdNk3-Mu4","executionInfo":{"status":"ok","timestamp":1695187112578,"user_tz":240,"elapsed":2,"user":{"displayName":"Imran Ahmad","userId":"08683678182734146579"}}},"execution_count":18,"outputs":[]}]}