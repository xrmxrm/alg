{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Chapter 11\n","#### Advanced Sequential Modeling Algorithms"],"metadata":{"id":"tB4M5w2NtkUe"}},{"cell_type":"markdown","source":["# Part1- Coding Autoencoders\n","we'll employ an autoencoder to reproduce these handwritten digits. The unique feature of autoencoders is their training mechanism: the input and the target output are the same image. Let's break this down.\n","First, there is the training phase, where the following steps occur:\n","1.\tThe MNIST images are provided to the autoencoder.\n","2.\tThe encoder segment compresses these images into a condensed latent representation.\n","3.\tThe decoder segment then tries to restore the original image from this representation. By iterating over this process, the autoencoder acquires the nuances of compressing and reconstructing, capturing the core patterns of the handwritten digits.\n","\n","Second, there is the reconstruction phase:\n","1.\tWith the model trained, when we feed it new images of handwritten digits, the autoencoder will first encode them into its internal representation.\n","2.\tThen, decoding this representation will yield a reconstructed image, which, if the training was successful, should closely match the original.\n","With the autoencoder effectively trained on the MNIST dataset, it becomes a powerful tool to process and reconstruct handwritten digit images.\n"],"metadata":{"id":"ZDIgwQN4vusg"}},{"cell_type":"markdown","source":["1. Import Necessary Libraries\n","Firstly, we need to ensure all the required libraries are imported."],"metadata":{"id":"eIUi8cybsRDq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmz50ztkr5b7"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","source":["2. Load the MNIST Data\n","We will load the MNIST dataset directly from TensorFlow's datasets module."],"metadata":{"id":"FYpRZ65HscSP"}},{"cell_type":"code","source":["# Load dataset\n","(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n","\n","# Normalize data to range [0, 1]\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n"],"metadata":{"id":"Tb7WB-Pxscym"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Define the Model\n","This section remains mostly unchanged."],"metadata":{"id":"lpBE_11ksplc"}},{"cell_type":"code","source":["# Define the autoencoder model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.Dense(784, activation='sigmoid'),\n","    tf.keras.layers.Reshape((28, 28))\n","])\n"],"metadata":{"id":"HSMKabJmsqEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Compile the Model\n","The model compilation stage."],"metadata":{"id":"LKKF_iJnsyu6"}},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='adam')\n"],"metadata":{"id":"tPjf0Li9szkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Train the Model\n","Training the autoencoder on the MNIST dataset."],"metadata":{"id":"BvXFAoWOs8KX"}},{"cell_type":"code","source":["model.fit(x_train, x_train, epochs=10, batch_size=128, validation_data=(x_test, x_test))\n"],"metadata":{"id":"Q5AahDurs8jg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Prediction\n","Obtain the encoded and decoded data."],"metadata":{"id":"zaE07GqBtHuw"}},{"cell_type":"code","source":["# For an autoencoder, the encoder and decoder parts are usually separate.\n","# Here, the entire autoencoder is used for encoding and decoding.\n","encoded_data = model.predict(x_test)\n","decoded_data = model.predict(encoded_data)\n"],"metadata":{"id":"zwiUMRARtIQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Visualization\n","Visualize the original and reconstructed images."],"metadata":{"id":"sz5v0TyrtYIJ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"6fhD5cCqthKP"}},{"cell_type":"code","source":["# Display original and reconstructed images\n","n = 10\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    # Original images\n","    ax = plt.subplot(2, n, i + 1)\n","    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","    # Reconstructed images\n","    ax = plt.subplot(2, n, i + 1 + n)\n","    plt.imshow(decoded_data[i].reshape(28, 28), cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","plt.show()\n"],"metadata":{"id":"vXgTE3UutYju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part2- Self Attention\n","Here's a simplified version of how the self-attention mechanism can be implemented:"],"metadata":{"id":"oIjzgv0curhp"}},{"cell_type":"markdown","source":["Importing necessary libraries"],"metadata":{"id":"A4hr7JBTxj9k"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"ti2qHavTuqom"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining the self-attention function"],"metadata":{"id":"Gk9YgoMIwafk"}},{"cell_type":"code","source":["def self_attention(Q, K, V):\n","    \"\"\"\n","    Q: Query matrix\n","    K: Key matrix\n","    V: Value matrix\n","    \"\"\"\n","\n","    # Calculate the attention weights\n","    attention_weights = np.matmul(Q, K.T)\n","\n","    # Apply the softmax to get probabilities\n","    attention_probs = np.exp(attention_weights) / np.sum(np.exp(attention_weights), axis=1, keepdims=True)\n","\n","    # Multiply the probabilities with the value matrix to get the output\n","    output = np.matmul(attention_probs, V)\n","\n","    return output\n"],"metadata":{"id":"nIgKGE-qwa7M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example Usage:\n","Initialize matrices"],"metadata":{"id":"SaAHiLslw0DQ"}},{"cell_type":"code","source":["Q = np.array([[1, 0, 1], [0, 2, 0], [1, 1, 0]])  # Example Query\n","K = np.array([[1, 0, 1], [0, 2, 0], [1, 1, 0]])  # Key matrix\n","V = np.array([[0, 2, 0], [1, 0, 1], [0, 1, 2]])  # Value matrix\n"],"metadata":{"id":"QEDephIDw0V1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the output using the self_attention function"],"metadata":{"id":"mkDQ4UpQw6I5"}},{"cell_type":"code","source":["output = self_attention(Q, K, V)\n"],"metadata":{"id":"WC6Ir-mcw6im"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Display the result"],"metadata":{"id":"cgsEYNa2w8jP"}},{"cell_type":"code","source":["print(output)"],"metadata":{"id":"ygaNvOYAw89w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3kH1ha9vxpT4"},"execution_count":null,"outputs":[]}]}